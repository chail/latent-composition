<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
    <title>Latent Composition</title>

    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/javascript" src="jquery.mlens-1.0.min.js"></script>
    <script type="text/javascript" src="jquery.js"></script>
    <style>
        body {
            font-family: 'Open-Sans', sans-serif;
            font-weight: 300;
            background-color: #fff;
        }

        .content {
            width: 1000px;
            padding: 25px 50px;
            margin: 25px auto;
            background-color: white;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }

        .contentblock {
            width: 950px;
            margin: 0 auto;
            padding: 0;
            border-spacing: 25px 0;
        }

        .contentblock td {
            background-color: #fff;
            padding: 25px 50px;
            vertical-align: top;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }

        a,
        a:visited {
            color: #224b8d;
            font-weight: 300;
        }

        #authors {
            text-align: center;
            margin-bottom: 20px;
        }

        #conference {
            text-align: center;
            margin-bottom: 20px;
            font-style: italic;
        }

        #authors a {
            margin: 0 10px;
        }

        h1 {
            text-align: center;
            font-size: 35px;
            font-weight: 300;
        }

        h2 {
            font-size: 30px;
            font-weight: 300;
        }

        code {
            display: block;
            padding: 10px;
            margin: 10px 10px;
        }

        p {
            line-height: 25px;
            text-align: justify;
        }

        p code {
            display: inline;
            padding: 0;
            margin: 0;
        }

        #teasers {
            margin: 0 auto;
        }

        #teasers td {
            margin: 0 auto;
            text-align: center;
            padding: 5px;
        }

        #teasers img {
            width: 250px;
        }

        #results img {
            width: 133px;
        }

        #seeintodark {
            margin: 0 auto;
        }

        #sift {
            margin: 0 auto;
        }

        #sift img {
            width: 250px;
        }

        .downloadpaper {
            padding-left: 20px;
            float: right;
            text-align: center;
        }

        .downloadpaper a {
            font-weight: bold;
            text-align: center;
        }

        #demoframe {
            border: 0;
            padding: 0;
            margin: 0;
            width: 100%;
            height: 340px;
        }

        #feedbackform {
            border: 1px solid #ccc;
            margin: 0 auto;
            border-radius: 15px;
        }

        #eyeglass {
            height: 530px;
        }

        #eyeglass #wrapper {
            position: relative;
            height: auto;
            margin: 0 auto;
            float: left;
            width: 800px;
        }

        #mitnews {
            font-weight: normal;
            margin-top: 20px;
            font-size: 14px;
            width: 220px;
        }

        #mitnews a {
            font-weight: normal;
        }

        .teaser-img {
            width: 80%;
						display: block;
						margin-left: auto;
						margin-right: auto;
        }
        .teaser-gif {
						display: block;
						margin-left: auto;
						margin-right: auto;
        }
        .summary-img {
            width: 100%;
						display: block;
						margin-left: auto;
						margin-right: auto;
        }

        .iframe {
            width: 100%;
            height: 125%
        }

      .container {
        display: flex;
        align-items: center;
        justify-content: center
      }
      .image {
        flex-basis: 40%
      }
      .text {
        font-size: 20px;
        padding-left: 20px;
      }

    </style>
    <!-- Global site tag (gtag.js) - Google Analytics -->
		<!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98008272-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-98008272-2');
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
		-->

</head>

<body>

    <div class="content">
			<h1>Using latent space regression to analyze and leverage compositionality in GANs</h1>
        <p id="authors">
            <a href="http://people.csail.mit.edu/lrchai/">Lucy Chai</a>
						<a href="http://people.csail.mit.edu/jwulff/">Jonas Wulff</a>
            <a href="http://web.mit.edu/phillipi/">Phillip Isola</a><br>
            <!-- <strong>MIT Computer Science and Artificial Intelligence Laboratory</strong> -->
            MIT Computer Science and Artificial Intelligence Laboratory
						<br><i>International Conference on Learning Representations 2021</i>
        </p>
				<font size="+2">
					<p style="text-align: center;">
						<a href="http://arxiv.org/abs/2103.10426" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://github.com/chail/latent-composition" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://colab.research.google.com/drive/1p-L2dPMaqMyr56TYoYmBJhoyIyBJ7lzH?usp=sharing" target="_blank">[Colab]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="bibtex.txt" target="_blank">[Bibtex]</a>
						<!--
						<a href="TODO: youtube link?" target="_blank">[Video]</a>
						-->
					</p>
					</font>
        <p>
            <img class='teaser-img' src='img/teaser.jpeg'></img>
        </p>

				<p><strong>Abstract: </strong>
				In recent years, Generative Adversarial Networks have become ubiquitous in both research and public perception, but how GANs convert an unstructured latent code to a high quality output is still an open question. In this work, we investigate regression into the latent space as a probe to understand the compositional properties of GANs. We find that combining the regressor and a pretrained generator provides a strong image prior, allowing us to create composite images from a collage of random image parts at inference time while maintaining global consistency. To compare compositional properties across different generators, we measure the trade-offs between reconstruction of the unrealistic input and image quality of the regenerated samples. We find that the regression approach enables more localized editing of individual image parts compared to direct editing in the latent space, and we conduct experiments to quantify this independence effect. Our method is agnostic to the semantics of edits, and does not require labels or predefined concepts during training. Beyond image composition, our method extends to a number of related applications, such as image inpainting or example-based image editing, which we demonstrate on several GANs and datasets, and because it uses only a single forward pass, it can operate in real-time. 
				</p>

        <br clear="all">
    </div>
    <div class="content" id="samples">

        <h2 style="text-align:center;">Summary</h2>

				<div class="container">
					<div class="image">
						<img src='img/teaser.gif'></img>
					</div>
					<div class="text">
						<p> We use a latent regressor network that learns from missing data for image composition and image completion.  The combination of the regressor network and a pretrained GAN forms an image prior to create realistic images despite unrealistic input. This animation briefly demonstrates some applications of our method.</p>
					</div>
				</div>
        <br>
        <hr>
        <p style="text-align: center;">Using the latent regression and pretrained GAN, we can create automatic collages and merge them into coherent composite images.</p>
        <img class='summary-img' src='img/website_composition.jpeg'></img>

        <br>
        <hr>
        <p style="text-align: center;">We demonstrate an application of editing and merging real images. <br>To better fit a specific image, we do a few seconds of initial optimization, and the remaining editing occurs in real-time.</p>
        <img class='summary-img' style="width:80%;" src='img/finetune_edit.jpeg'></img>
        <br>
        <hr>
        <p style="text-align: center;">We use the latent regressor to investigate the independently changeable parts that the GAN learns from data. For example, we visualize what regions of a given image change, when the outlined red portion is modified. The resulting variations show regions of the images that commonly vary together, which can be interpreted as a form of unsupervised part discovery.</p>
        <img class='summary-img' src='img/website_independence.jpeg'></img>

				<p style="text-align: center;"> More content (e.g. poster and more random samples) coming soon! </p>

    </div>      
    <div class="content" id="references">

        <h2>Reference</h2>

				<p>L Chai, J Wulff, P Isola. Using latent space regression to analyze and leverage compositionality in GANs. <br>International Conference on Learning Representations, 2021.</p>

        <code>
			@inproceedings{chai2021latent,<br>
				&nbsp;&nbsp;title={Using latent space regression to analyze and leverage compositionality in GANs.},<br>
				&nbsp;&nbsp;author={Chai, Lucy and Wulff, Jonas and Isola, Phillip},<br>
				&nbsp;&nbsp;booktitle={International Conference on Learning Representations},<br>
				&nbsp;&nbsp;year={2021}<br>
			 }
				</code>
    </div>      
    <div class="content" id="acknowledgements">
          <p><strong>Acknowledgements</strong>:
					We would like to thank David Bau, Richard Zhang, Tongzhou Wang, Luke Anderson, and Yen-Chen Lin for helpful discussions and feedback. Thanks to Antonio Torralba, Alyosha Efros, Richard Zhang, Jun-Yan Zhu, Wei-Chiu Ma, Minyoung Huh, and Yen-Chen Lin for permission to use their photographs. LC is supported by the National Science Foundation Graduate Research Fellowship under Grant No. 1745302. JW is supported by a grant from Intel Corp.
					 Recycling a familiar <a href="https://chail.github.io/patch-forensics/">template</a> ;).
    </div>
</body>

</html>
